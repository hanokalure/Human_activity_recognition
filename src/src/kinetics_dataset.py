"""
Kinetics-400 Dataset Loader for Phase 2 Daily Activities
========================================================

Loads Kinetics-400 videos for the 10 Phase 2 daily activities.
Compatible with the Phase 1 dataset structure and training pipeline.
"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import cv2
import numpy as np
from pathlib import Path
import json
from collections import defaultdict
import random

from kinetics_class_mapping import (
    get_kinetics_class_mapping, 
    get_all_activities,
    PHASE_1_ACTIVITIES,
    PHASE_2_ACTIVITIES
)

class KineticsVideoDataset(Dataset):
    """Dataset for loading Kinetics-400 videos for daily activities"""
    
    def __init__(self, video_root, video_list_file, transform=None, 
                 frames_per_clip=16, clip_duration=2.0):
        """
        Args:
            video_root: Path to Kinetics videos directory
            video_list_file: Path to kinetics400_val_list_videos.txt
            transform: Video transforms
            frames_per_clip: Number of frames to sample per video
            clip_duration: Duration in seconds for each clip
        """
        self.video_root = Path(video_root)
        self.transform = transform
        self.frames_per_clip = frames_per_clip
        self.clip_duration = clip_duration
        
        # Load class mappings
        self.kinetics_to_daily = get_kinetics_class_mapping()
        self.all_activities = get_all_activities()
        self.phase_2_activities = PHASE_2_ACTIVITIES
        
        # Create activity to index mapping (for 19 total classes)
        self.activity_to_idx = {activity: idx for idx, activity in enumerate(self.all_activities)}
        self.idx_to_activity = {idx: activity for activity, idx in self.activity_to_idx.items()}
        
        # Load video list and filter for our target activities
        self.video_data = self._load_video_list(video_list_file)
        
        print(f\"‚úÖ Loaded {len(self.video_data)} Kinetics videos for Phase 2\")\n        print(f\"üìä Target activities: {len(self.phase_2_activities)}\")\n        \n        # Print dataset statistics\n        self._print_dataset_stats()\n    \n    def _load_video_list(self, video_list_file):\n        \"\"\"Load and filter video list for target daily activities\"\"\"\n        video_data = []\n        activity_counts = defaultdict(int)\n        \n        with open(video_list_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) >= 2:\n                    video_filename = parts[0]\n                    class_id = int(parts[1])\n                    \n                    # Check if this class maps to one of our target activities\n                    if class_id in self.kinetics_to_daily:\n                        daily_activity = self.kinetics_to_daily[class_id]\n                        \n                        # Only include Phase 2 activities\n                        if daily_activity in self.phase_2_activities:\n                            video_path = self.video_root / video_filename\n                            \n                            # Check if video file exists\n                            if video_path.exists():\n                                video_data.append({\n                                    'path': video_path,\n                                    'activity': daily_activity,\n                                    'activity_idx': self.activity_to_idx[daily_activity],\n                                    'kinetics_class': class_id\n                                })\n                                activity_counts[daily_activity] += 1\n        \n        print(f\"\\nüìä Videos per activity:\")\n        for activity, count in sorted(activity_counts.items()):\n            print(f\"  ‚Ä¢ {activity}: {count} videos\")\n        \n        return video_data\n    \n    def _print_dataset_stats(self):\n        \"\"\"Print dataset statistics\"\"\"\n        activity_counts = defaultdict(int)\n        for item in self.video_data:\n            activity_counts[item['activity']] += 1\n        \n        print(f\"\\nüìà Dataset Statistics:\")\n        print(f\"  ‚Ä¢ Total videos: {len(self.video_data)}\")\n        print(f\"  ‚Ä¢ Activities: {len(activity_counts)}\")\n        print(f\"  ‚Ä¢ Frames per clip: {self.frames_per_clip}\")\n        \n        # Check for class imbalance\n        counts = list(activity_counts.values())\n        if counts:\n            min_count, max_count = min(counts), max(counts)\n            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n            print(f\"  ‚Ä¢ Class imbalance ratio: {imbalance_ratio:.2f}\")\n            \n            if imbalance_ratio > 3.0:\n                print(f\"  ‚ö†Ô∏è  High class imbalance detected!\")\n    \n    def __len__(self):\n        return len(self.video_data)\n    \n    def __getitem__(self, idx):\n        \"\"\"Load and process a video clip\"\"\"\n        video_info = self.video_data[idx]\n        video_path = video_info['path']\n        label = video_info['activity_idx']\n        \n        try:\n            # Load video frames\n            frames = self._load_video_frames(video_path)\n            \n            if frames is None or len(frames) == 0:\n                # Return a random different video if this one fails\n                return self.__getitem__(random.randint(0, len(self.video_data) - 1))\n            \n            # Apply transforms\n            if self.transform:\n                frames = self.transform(frames)\n            \n            return frames, label\n            \n        except Exception as e:\n            print(f\"Error loading video {video_path}: {e}\")\n            # Return a random different video if this one fails\n            return self.__getitem__(random.randint(0, len(self.video_data) - 1))\n    \n    def _load_video_frames(self, video_path):\n        \"\"\"Load frames from video file using OpenCV\"\"\"\n        cap = cv2.VideoCapture(str(video_path))\n        \n        if not cap.isOpened():\n            print(f\"Warning: Could not open video {video_path}\")\n            return None\n        \n        frames = []\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        if fps <= 0 or total_frames <= 0:\n            cap.release()\n            return None\n        \n        # Calculate frame sampling\n        clip_frames = int(fps * self.clip_duration)\n        if clip_frames > total_frames:\n            clip_frames = total_frames\n        \n        # Sample frames evenly across the clip\n        if clip_frames > self.frames_per_clip:\n            frame_indices = np.linspace(0, clip_frames - 1, self.frames_per_clip, dtype=int)\n        else:\n            frame_indices = list(range(clip_frames))\n            # Pad with last frame if needed\n            while len(frame_indices) < self.frames_per_clip:\n                frame_indices.append(frame_indices[-1])\n        \n        # Load selected frames\n        for frame_idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            \n            if ret:\n                # Convert BGR to RGB\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frames.append(frame)\n            else:\n                # Use last valid frame if available\n                if frames:\n                    frames.append(frames[-1])\n        \n        cap.release()\n        \n        # Ensure we have the right number of frames\n        while len(frames) < self.frames_per_clip:\n            if frames:\n                frames.append(frames[-1])\n            else:\n                # Create a black frame if no valid frames\n                frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n        \n        return np.array(frames[:self.frames_per_clip])\n\ndef get_kinetics_transforms(train=True, input_size=224):\n    \"\"\"Get video transforms for Kinetics data (matching Phase 1 preprocessing)\"\"\"\n    if train:\n        transform = transforms.Compose([\n            transforms.Lambda(lambda x: torch.FloatTensor(x)),  # Convert to tensor\n            transforms.Lambda(lambda x: x.permute(3, 0, 1, 2)),  # (H,W,C,T) -> (C,T,H,W)  \n            transforms.Lambda(lambda x: torch.nn.functional.interpolate(\n                x, size=(input_size, input_size), mode='bilinear', align_corners=False\n            )),  # Resize\n            transforms.Lambda(lambda x: x / 255.0),  # Normalize to [0,1]\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n        ])\n    else:\n        transform = transforms.Compose([\n            transforms.Lambda(lambda x: torch.FloatTensor(x)),\n            transforms.Lambda(lambda x: x.permute(3, 0, 1, 2)),\n            transforms.Lambda(lambda x: torch.nn.functional.interpolate(\n                x, size=(input_size, input_size), mode='bilinear', align_corners=False\n            )),\n            transforms.Lambda(lambda x: x / 255.0),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    return transform\n\ndef get_kinetics_dataloaders(video_root, video_list_file, batch_size=8, \n                           frames_per_clip=16, num_workers=0, val_split=0.2):\n    \"\"\"Create train/validation dataloaders for Kinetics data\"\"\"\n    \n    # Load full dataset\n    full_dataset = KineticsVideoDataset(\n        video_root=video_root,\n        video_list_file=video_list_file,\n        transform=None,  # Will add transforms later\n        frames_per_clip=frames_per_clip\n    )\n    \n    if len(full_dataset) == 0:\n        raise ValueError(\"No valid videos found in Kinetics dataset!\")\n    \n    # Split dataset\n    total_size = len(full_dataset)\n    val_size = int(val_split * total_size)\n    train_size = total_size - val_size\n    \n    train_dataset, val_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size]\n    )\n    \n    # Add transforms\n    train_transform = get_kinetics_transforms(train=True)\n    val_transform = get_kinetics_transforms(train=False)\n    \n    train_dataset.dataset.transform = train_transform\n    val_dataset.dataset.transform = val_transform\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=torch.cuda.is_available(),\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=torch.cuda.is_available()\n    )\n    \n    print(f\"\\n‚úÖ Created Kinetics dataloaders:\")\n    print(f\"  ‚Ä¢ Train: {len(train_loader)} batches ({len(train_dataset)} videos)\")\n    print(f\"  ‚Ä¢ Val: {len(val_loader)} batches ({len(val_dataset)} videos)\")\n    \n    # Return class information\n    class_info = {\n        'num_classes': len(full_dataset.all_activities),\n        'class_names': full_dataset.all_activities,\n        'activity_to_idx': full_dataset.activity_to_idx,\n        'phase_2_activities': full_dataset.phase_2_activities\n    }\n    \n    return train_loader, val_loader, class_info\n\nif __name__ == \"__main__\":\n    # Test the dataset loader\n    video_root = r\"C:\\ASH_PROJECT\\data\\kinetics400\\videos_val\"\n    video_list_file = r\"C:\\ASH_PROJECT\\data\\kinetics400\\kinetics400_val_list_videos.txt\"\n    \n    print(\"üß™ Testing Kinetics dataset loader...\")\n    \n    try:\n        train_loader, val_loader, class_info = get_kinetics_dataloaders(\n            video_root=video_root,\n            video_list_file=video_list_file,\n            batch_size=2,\n            frames_per_clip=8,  # Smaller for testing\n            num_workers=0\n        )\n        \n        print(f\"\\n‚úÖ Dataset test successful!\")\n        print(f\"üìä Class info: {class_info['num_classes']} classes\")\n        \n        # Test loading one batch\n        print(\"\\nüîç Testing batch loading...\")\n        for batch_idx, (videos, labels) in enumerate(train_loader):\n            print(f\"  ‚Ä¢ Batch shape: {videos.shape}\")\n            print(f\"  ‚Ä¢ Labels: {labels}\")\n            print(f\"  ‚Ä¢ Video range: [{videos.min():.3f}, {videos.max():.3f}]\")\n            break\n        \n        print(\"\\nüéâ All tests passed!\")\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Error: {e}\")\n        import traceback\n        traceback.print_exc()"